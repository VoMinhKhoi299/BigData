# - D√πng token trong .env (SPOTIFY_BEARER)
# - H·ªó tr·ª£ resume: n·∫øu raw_json ƒë√£ c√≥ th√¨ kh√¥ng g·ªçi l·∫°i
# - T√¥n tr·ªçng RPS + jitter ƒë∆°n gi·∫£n
# - Ghi ra CSV data_raw/spotify_viral_global_vn.csv v·ªõi schema c·ªë ƒë·ªãnh

import os, json, time, argparse, datetime as dt, random
from pathlib import Path
from typing import List, Dict, Any
import requests, pandas as pd
from dotenv import load_dotenv
from tqdm import tqdm

# ----------- C·∫•u h√¨nh c∆° b·∫£n -----------
AUTH_BASE = "https://charts-spotify-com-service.spotify.com/auth/v0/charts"
UA        = "Mozilla/5.0 (Macintosh; Intel Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17 Safari/605.1.15"

RAW_DIR   = Path("data_raw/raw_json")
CSV_OUT   = Path("data_raw/spotify_viral_global_vn.csv")
LOG_FILE  = Path("logs/crawl.log")

RAW_DIR.mkdir(parents=True, exist_ok=True)
LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

# ----------- Ti·ªán √≠ch ng·∫Øn g·ªçn -----------
def log(msg: str):
    ts = dt.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{ts}] {msg}\n")

def load_session() -> requests.Session:
    load_dotenv()
    token = os.getenv("SPOTIFY_BEARER", "").strip()
    if not token:
        raise SystemExit("Thi·∫øu SPOTIFY_BEARER trong .env")
    s = requests.Session()
    s.headers.update({"User-Agent": UA, "authorization": f"Bearer {token}"})
    return s

def sleep_rps(rps: float, jitter: float = 0.35):
    base = 1.0 / max(rps, 0.01)
    time.sleep(max(0.0, base + random.uniform(-base*jitter, base*jitter)))

def fetch_json(s: requests.Session, url: str, raw_path: Path, resume: bool) -> Dict[str, Any]:
    """ƒê·ªçc cache n·∫øu c√≥; g·ªçi m·∫°ng ƒë∆°n gi·∫£n v·ªõi x·ª≠ l√Ω 401/404/429."""
    if resume and raw_path.exists():
        with open(raw_path, "r", encoding="utf-8") as f:
            return json.load(f)

    r = s.get(url, timeout=30)
    if r.status_code == 401:
        raise SystemExit("401 Unauthorized ‚Äî refresh token trong .env r·ªìi ch·∫°y l·∫°i v·ªõi --resume")
    if r.status_code == 404:
        log(f"404 {url}")
        return {"__status": 404}
    if r.status_code == 429:
        wait = int(r.headers.get("Retry-After", "5"))
        log(f"‚ö†Ô∏è 429 {url} ‚Äî ƒë·ª£i {wait}s")
        time.sleep(wait)
        r = s.get(url, timeout=30)  # th·ª≠ l·∫°i 1 l·∫ßn
    r.raise_for_status()

    data = r.json()
    with open(raw_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
    return data

def parse_entries(data: Dict[str, Any], region: str, day: str) -> List[Dict[str, Any]]:
    """R√∫t g·ªçn: ch·ªâ gi·ªØ c√°c tr∆∞·ªùng ph·ª•c v·ª• ETL/Th·ªëng k√™; KH√îNG c√≥ week_*."""
    rows = []
    entries = data.get("entries") or []
    retrieved_at = dt.datetime.utcnow().isoformat(timespec="seconds") + "Z"
    for e in entries:
        tm = e.get("trackMetadata") or {}
        ce = e.get("chartEntryData") or {}
        if not tm:  # b·ªè entry kh√¥ng ph·∫£i b√†i h√°t
            continue

        # gh√©p danh s√°ch ngh·ªá sƒ©
        names, uris = [], []
        for a in (tm.get("artists") or []):
            if a.get("name"): names.append(a["name"])
            u = a.get("spotifyUri") or a.get("uri")
            if u: uris.append(u)

        # l·∫•y track_id t·ª´ URI n·∫øu c√≥ (spotify:track:<id>)
        track_uri = tm.get("trackUri") or tm.get("uri")
        track_id = None
        if track_uri:
            parts = track_uri.split(":")
            if len(parts) >= 3 and parts[-2] == "track": track_id = parts[-1]

        rows.append({
            # ƒë·ªãnh danh b·∫£n ghi
            "chart_type": "viral_daily",
            "period": "daily",
            "date": day,
            "region": region.lower(),
            "retrieved_at_utc": retrieved_at,

            # x·∫øp h·∫°ng
            "rank": ce.get("currentRank"),
            "previous_rank": ce.get("previousRank"),
            "weeks_on_chart": ce.get("weeksOnChart"),

            # m√¥ t·∫£ b√†i h√°t
            "track_name": tm.get("trackName"),
            "artists": ", ".join(names) if names else None,
            "artist_uris": ";".join(uris) if uris else None,
            "track_id": track_id,
            "release_date": tm.get("releaseDate"),
        })
    return rows

def write_csv(rows: List[Dict[str, Any]]):
    if not rows: return
    df = pd.DataFrame(rows)
    cols = ["chart_type","period","date","region","retrieved_at_utc",
            "rank","previous_rank","weeks_on_chart",
            "track_name","artists","artist_uris","track_id","release_date"]
    df = df.reindex(columns=cols)
    # append c√≥/kh√¥ng header tu·ª≥ file t·ªìn t·∫°i
    df.to_csv(CSV_OUT, mode="a" if CSV_OUT.exists() else "w",
              header=not CSV_OUT.exists(), index=False, encoding="utf-8-sig")

def each_day(start: dt.date, end: dt.date):
    cur = start
    while cur <= end:
        yield cur.strftime("%Y-%m-%d")
        cur += dt.timedelta(days=1)

# ----------- Ch∆∞∆°ng tr√¨nh ch√≠nh -----------
def crawl(regions: List[str], start: str, end: str, rps: float, resume: bool):
    s = load_session()
    days = list(each_day(dt.datetime.strptime(start, "%Y-%m-%d").date(),
                         dt.datetime.strptime(end,   "%Y-%m-%d").date()))
    tasks = [(d, r) for d in days for r in regions]
    pbar = tqdm(total=len(tasks), desc="üéß Viral daily")

    for day, region in tasks:
        url = f"{AUTH_BASE}/viral-{region.lower()}-daily/{day}"
        raw = RAW_DIR / f"{day}_{region.lower()}_viral_daily.json"
        try:
            data = fetch_json(s, url, raw, resume=resume)
            if data.get("__status") != 404:
                write_csv(parse_entries(data, region, day))
        except Exception as ex:
            log(f"ERR {url}: {ex}")
        finally:
            pbar.update(1)
            sleep_rps(rps)
    pbar.close()

def main():
    ap = argparse.ArgumentParser(description="Crawl Spotify VIRAL daily -> CSV")
    ap.add_argument("--regions", nargs="+", default=["global","vn"])   # ch·ªçn khu v·ª±c
    ap.add_argument("--start", required=True, help="YYYY-MM-DD")       # ng√†y b·∫Øt ƒë·∫ßu
    ap.add_argument("--end",   required=True, help="YYYY-MM-DD")       # ng√†y k·∫øt th√∫c
    ap.add_argument("--rps", type=float, default=0.30)                 # requests/second
    ap.add_argument("--resume", action="store_true")                   # b·ªè qua URL ƒë√£ c√≥ raw JSON
    args = ap.parse_args()

    print(f"üåç REGIONS: {args.regions}")
    print(f"üìÜ DATE RANGE: {args.start} -> {args.end}")
    print(f"üéöÔ∏è RPS={args.rps} | RESUME={args.resume}")
    crawl(args.regions, args.start, args.end, args.rps, args.resume)

if __name__ == "__main__":
    main()
